<p>http://en.wikipedia.org/wiki/Naive_Bayes_classifier</p><p>&nbsp;</p><p>&nbsp;</p><p>Abstractly, the probability model for a classifier is a conditional model 模型：<br /></p> <dl><dd><img alt="p(C \vert F_1,\dots,F_n)\," src="http://upload.wikimedia.org/math/4/7/a/47a22e381fe04c90c2115d4ac369f590.png" /></dd></dl>&nbsp;可以展开为<dl><dd><img alt="p(C \vert F_1,\dots,F_n) = \frac{p(C) \ p(F_1,\dots,F_n\vert C)}{p(F_1,\dots,F_n)}. \," src="http://upload.wikimedia.org/math/3/1/7/3174021f44ba0d31f6ede772624c5523.png" /></dd></dl> <p>In plain English the above equation can be written as</p> <dl><dd><img alt="\mbox{posterior} = \frac{\mbox{prior} \times \mbox{likelihood}}{\mbox{evidence}}. \," src="http://upload.wikimedia.org/math/b/7/e/b7ee688183182869d44cfaa3b22f5eb6.png" /></dd></dl><p>&nbsp;</p><p>关键是计算分子，因为分母为常数</p><p>而分子可以展开为<br /></p><p>The numerator is equivalent to the <a href="http://en.wikipedia.org/wiki/Joint_probability" title="Joint probability">joint probability</a> model </p><dl><dd><img alt="p(C, F_1, \dots, F_n)\," src="http://upload.wikimedia.org/math/0/4/4/044dfd3e1b822193af59618fa30640b2.png" /></dd></dl> <p>which can be rewritten as follows, using repeated applications of the definition of <a href="http://en.wikipedia.org/wiki/Conditional_probability" title="Conditional probability">conditional probability</a>:</p> <dl><dd><img alt="p(C, F_1, \dots, F_n)\," src="http://upload.wikimedia.org/math/0/4/4/044dfd3e1b822193af59618fa30640b2.png" /></dd></dl> <dl><dd> <dl><dd><img alt="= p(C) \ p(F_1,\dots,F_n\vert C)" src="http://upload.wikimedia.org/math/f/9/a/f9afa278bebc99dcf98d0c128c0bc4f6.png" /></dd></dl> </dd></dl> <dl><dd> <dl><dd><img alt="= p(C) \ p(F_1\vert C) \ p(F_2,\dots,F_n\vert C, F_1)" src="http://upload.wikimedia.org/math/8/4/4/84456edba2adc600eba6a60630263fee.png" /></dd></dl> </dd></dl> <dl><dd> <dl><dd><img alt="= p(C) \ p(F_1\vert C) \ p(F_2\vert C, F_1) \ p(F_3,\dots,F_n\vert C, F_1, F_2)" src="http://upload.wikimedia.org/math/7/c/6/7c65e02bf0e3ea028db57b2cd36fb47d.png" /></dd></dl> </dd></dl> <dl><dd> <dl><dd><img alt="= p(C) \ p(F_1\vert C) \ p(F_2\vert C, F_1) \ p(F_3\vert C, F_1, F_2) \ p(F_4,\dots,F_n\vert C, F_1, F_2, F_3)" src="http://upload.wikimedia.org/math/4/d/0/4d040737ebbea0caace384f9d7c8f3b5.png" /></dd></dl> </dd></dl> <dl><dd> <dl><dd><img alt="= p(C) \ p(F_1\vert C) \ p(F_2\vert C, F_1) \ p(F_3\vert C, F_1, F_2) \ \dots p(F_n\vert C, F_1, F_2, F_3,\dots,F_{n-1})." src="http://upload.wikimedia.org/math/7/4/5/7455b4223ccb2aa4d3c1abdb205896d4.png" /></dd></dl> </dd></dl> <p>Now the "naive" <a href="http://en.wikipedia.org/wiki/Conditional_independence" title="Conditional independence">conditional independence</a> assumptions come into play: assume that each feature <em>F</em><sub><em>i</em></sub> is conditionally <a href="http://en.wikipedia.org/wiki/Statistical_independence" title="Statistical independence">independent</a> of every other feature <em>F</em><sub><em>j</em></sub> for <img alt="j\neq i" src="http://upload.wikimedia.org/math/c/c/f/ccf5f455f5de788b5d5eeaf339059de1.png" />. This means that</p> <dl><dd><img alt="p(F_i \vert C, F_j) = p(F_i \vert C)\," src="http://upload.wikimedia.org/math/8/3/0/83005704f3e8c5a3a325b4ded09708b7.png" /></dd></dl> <p>for <img alt="i\ne j" src="http://upload.wikimedia.org/math/3/d/2/3d27367ea16a2a7b40b3eb3172a32120.png" />, and so the joint model can be expressed as</p> <dl><dd><img alt="p(C, F_1, \dots, F_n) = p(C) \ p(F_1\vert C) \ p(F_2\vert C) \ p(F_3\vert C) \ \cdots\," src="http://upload.wikimedia.org/math/a/b/e/abe40736c314a87f0cf2e470f11d838d.png" /></dd></dl> <dl><dd><img alt="= p(C) \prod_{i=1}^n p(F_i \vert C).\," src="http://upload.wikimedia.org/math/a/b/9/ab90205c26b3ab9caa56efdb3fd29d85.png" /></dd></dl> <p>This means that under the above independence assumptions, the conditional distribution over the class variable <em>C</em> can be expressed like this:这里是最终的分子：</p> <dl><dd><img alt="p(C \vert F_1,\dots,F_n) = \frac{1}{Z}  p(C) \prod_{i=1}^n p(F_i \vert C)" src="http://upload.wikimedia.org/math/b/a/e/bae76a0ac40ee656990387fe8f79d0bf.png" /></dd></dl>&nbsp;<h2><span id="Constructing_a_classifier_from_the_probability_model">Constructing a classifier from the probability model</span></h2> <p>The discussion so far has derived the independent feature model, that is, the naive Bayes <a href="http://en.wikipedia.org/w/index.php?title=Probability_model&amp;action=edit&amp;redlink=1" title="Probability model (page does not exist)">probability model</a>. The naive Bayes <a href="http://en.wikipedia.org/wiki/Classifier" title="Classifier">classifier</a> combines this model with a <a href="http://en.wikipedia.org/wiki/Decision_rule" title="Decision rule">decision rule</a>. One common rule is to pick the hypothesis that is most probable; this is known as the <em><a href="http://en.wikipedia.org/wiki/Maximum_a_posteriori" title="Maximum a posteriori">maximum a posteriori</a></em> or <em>MAP</em> decision rule. The corresponding classifier is the function classify defined as follows:贝叶斯分类器的构造，通常为使用最大似然优化以下函数</p> <dl><dd><img alt="\mathrm{classify}(f_1,\dots,f_n) = \underset{c}{\operatorname{argmax}} \ p(C=c) \displaystyle\prod_{i=1}^n p(F_i=f_i\vert C=c)." src="http://upload.wikimedia.org/math/c/2/e/c2e227dfe0979e43cf06bfa318652dd3.png" /></dd></dl><p>更详细的判别函数，及参数估计（最大似然及贝叶斯参数估计）的推导最好看书， 推荐《模式分类》<br /></p><dl><dt><br /></dt></dl><p>&nbsp;</p>